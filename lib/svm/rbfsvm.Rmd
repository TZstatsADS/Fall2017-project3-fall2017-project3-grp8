---
title: "R Notebook"
output: html_notebook
---
```{r}
# load library
library(e1071)

### GIST features ###
# load data
setwd("~/Documents/Fall2017-project3-fall2017-project3-grp8/data/our_data/training_set")
X<-read.csv("features_GIST.csv",header=FALSE,as.is=TRUE)
X.train<-X[indicator,]
X.test<-X[-indicator,]
# tune parameters and tune control
par.list = list(cost = c(0.1,1,3,5,8,10),
                gamma = c(0.001, 0.005, 0.01, 0.05, 1, 5, 8, 10))
k = tune.control(cross = 5)

# tune svm with multiple classes using the one-versus-one approach
tune.out = tune(svm, train.x = X.train, train.y = y.train, kernel = "radial",
                scale = FALSE, ranges = par.list, tunecontrol = k)

tune.out$best.parameters # cost = 5, gamma =5
tune.out$best.performance # 0.267
performances<-tune.out$performances
save(tune.out, file="fit_train_svmRBF_GIST.RData")

# train the best model on the whole training set
bestmod = tune.out$best.model
tm_train <- system.time(pred <- predict(bestmod, X.test))
save(pred, file="pred_test_svmRBF_GIST.RData")
sum(pred != y.test)/length(y.test) # 0.242
cat("Time for training model=", tm_train[1], "s \n") # 1.031s


### HOG features ###
# load data
X<-read.csv("features_HOG.csv",header=TRUE,as.is=TRUE)[,-1]
X.train<-X[indicator,]
X.test<-X[-indicator,]

# tune parameters and tune control
par.list = list(cost = c(0.001, 0.01, 0.1, 1, 2, 3, 5, 8, 10),
                 gamma = c(0.01, 0.1, 1, 10, 100))
k = tune.control(cross = 5)

# tune svm with multiple classes using the one-versus-one approach
tune.out = tune(svm, train.x = X.train, train.y = y.train, kernel = "radial",
                scale = FALSE, ranges = par.list, tunecontrol = k)

tune.out$best.parameters # cost = 8, gamma =100
tune.out$best.performance # 0.223
performances<-tune.out$performances
save(tune.out, file="fit_train_svmRBF_HOG.RData")

# train the best model on the whole training set
bestmod = tune.out$best.model
tm_train <- system.time(pred <- predict(bestmod, X.test))
save(pred, file="pred_test_svmRBF_HOG.RData")
sum(pred != y.test)/length(y.test) # 0.21
cat("Time for training model=", tm_train[1], "s \n") # 0.085s

### SIFT ###
# load data
X<-read.csv("sift_train.csv",header=TRUE,as.is=TRUE)[,-1]
X.train<-X[indicator,]
X.test<-X[-indicator,]

# tune parameters and tune control
par.list = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10),
                 gamma = c(0.01, 0.1, 1, 10, 100))
k = tune.control(cross = 5)

# tune svm with multiple classes using the one-versus-one approach
tune.out = tune(svm, train.x = X.train, train.y = y.train, kernel = "radial",
                scale = FALSE, ranges = par.list, tunecontrol = k)

tune.out$best.parameters # cost = 5, gamma =100
tune.out$best.performance # 0.184
performances<-tune.out$performances
save(tune.out, file="fit_train_svmRBF_SIFT.RData")

# train the best model on the whole training set
bestmod = tune.out$best.model
tm_train <- system.time(pred <- predict(bestmod, X.test))
save(pred, file="pred_test_svmRBF_SIFT.RData")
sum(pred != y.test)/length(y.test) # 0.176
cat("Time for training model=", tm_train[1], "s \n") # 5.81s
```